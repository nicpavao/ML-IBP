{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial Notebook\n",
    "This is a little jupyter notebook to explore the functionality of each module in the code box. First let's start with the model building code\n",
    "\n",
    "## Building\n",
    "This is the most important module -- here we actually build constructor classes for the model, and delineate between the model which is trained, and the model which does inference. \n",
    "\n",
    "The module is essentially split into 3 section:\n",
    "\n",
    "1 & 2) Dataloading & \n",
    "\n",
    "3 & 4) Tech Stack: Embedding and Encoder/Decoder Stacks\n",
    "\n",
    "5 & 6) Models: KernelModel (for training) & Projection Model (for inference)\n",
    "\n",
    "This section of the notebook just explicates the important functions that we use in the implementation for each. Let's begin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from building import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloading and Tokenization\n",
    "\n",
    "This is pretty self-explanatory: there's a tokenizer class (CustomTokenizer), and a Dataset class (FIRE6Dataset). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "# Dataloader & Tokenizer\n",
    "# ---------------------\n",
    "tokenizer = CustomTokenizer()\n",
    "data = FIRE6Dataset(\"../data/train\", tokenizer, max_len=1000)\n",
    "batchsize = 5\n",
    "dataloader = DataLoader(data,batch_size=batchsize,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer methods, tokenize and detokenize, will be useful for our inference model. But really all you need to know is that data is an object that takes the strings in the .dat files, and turns them into a bunch of padded tensors of tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tech Stack\n",
    "\n",
    "You can in principle use the PyTorch classes for this section, but I find it useful to tease out the actualy building blocks of the model. This way it's very clear what's going on inside the KernelModel, and you can play around with individual blocks. \n",
    "\n",
    "This is essentially the flow of the KernelModel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_model_forward(self,src, tgt):\n",
    "    # embed the input sequence in model space\n",
    "    x0 = self.embedding(src)\n",
    "    x1 = self.positional_encoding(x0)\n",
    "    memory = self.encoder_stack(x1)\n",
    "\n",
    "    # embed a padded sequence upto the n-th token in output space\n",
    "    y0 = self.embedding(tgt)\n",
    "    tgt_input = self.positional_encoding(y0)\n",
    "\n",
    "    # throw this into a decoder that hides all the tokens after the n-th token\n",
    "    tgt_output = self.decoder_stack(tgt_input, memory, self.causal_mask)\n",
    "    logits = self.fc_out(tgt_output)\n",
    "\n",
    "    # return the logits -- NB: these are not probabilities!\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the blocks of functions are defined in sections 3 & 4 of the code module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "\n",
    "This is the meat and potatos. Here we can define a model that is a blank canvas using our KernelModel class. After we define the hyperparameters and use them to instantiate the model, we can do 1 of 2 things:\n",
    "\n",
    "1) train it and mold the paramters to our likin with data\n",
    "\n",
    "2) load a model stat dictionary from a .pth file that is consistent with this instance of the KernelModel\n",
    "\n",
    "Let's see this in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicpavao/Repos/MyRepos/ML-IBP/venv/lib/python3.13/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------------------\n",
    "# Hyperparamters\n",
    "# ---------------------\n",
    "\n",
    "vocab_size = len(CustomTokenizer().vocab)\n",
    "d_model = 16\n",
    "d_ffn = 16\n",
    "nhead = 4\n",
    "encoder_layers = 4\n",
    "decoder_layers = 4\n",
    "dropout = 0.1\n",
    "PAD_TOKEN_ID = 0\n",
    "\n",
    "# ---------------------\n",
    "# Model\n",
    "# ---------------------\n",
    "\n",
    "model = KernelModel(vocab_size,d_model,d_ffn,nhead,encoder_layers,decoder_layers,dropout)\n",
    "model.load_state_dict(torch.load(\"../models/test_model.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run some dummy data through this model. The model takes as input a tensor with tokens in the vocabulary of our tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = torch.randint(15, (10,10))\n",
    "tgt = torch.randint(15, (10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-4.5918,  1.0286,  1.3364,  ..., -3.6634,  0.0891, -4.7492],\n",
       "         [-4.0936,  1.2809,  1.5021,  ..., -3.4740, -0.7971, -4.5059],\n",
       "         [-4.6641,  1.6571,  1.5042,  ..., -4.4009, -0.0971, -4.6453],\n",
       "         ...,\n",
       "         [-4.6669,  1.1684,  1.4334,  ..., -3.6470,  0.0199, -4.8630],\n",
       "         [-4.5707,  0.1531,  0.5614,  ..., -0.8732,  2.6705, -4.5953],\n",
       "         [-4.0723,  1.5911,  1.5782,  ..., -3.4980, -1.0484, -4.5811]],\n",
       "\n",
       "        [[-4.4722,  1.1234,  1.4237,  ..., -3.5539, -0.1702, -4.7558],\n",
       "         [-4.0286,  1.1482,  1.4078,  ..., -3.0399, -0.8441, -4.5372],\n",
       "         [-4.5493,  4.2288,  2.0093,  ..., -4.6730, -1.3699, -4.4064],\n",
       "         ...,\n",
       "         [-4.3903,  0.8531,  1.3447,  ..., -3.6983,  0.2342, -4.6064],\n",
       "         [-4.7878,  0.7533,  1.0412,  ..., -2.4487,  1.0180, -4.9894],\n",
       "         [-4.0476,  1.3867,  1.4238,  ..., -3.5978, -0.8200, -4.4834]],\n",
       "\n",
       "        [[-3.9507,  4.2087,  2.0719,  ..., -3.9265, -2.4222, -4.0103],\n",
       "         [-4.3250,  0.1591,  0.2106,  ...,  0.1431,  2.2754, -4.5771],\n",
       "         [-4.5566,  1.3628,  1.4086,  ..., -4.0434,  0.1748, -4.6627],\n",
       "         ...,\n",
       "         [-4.2886,  4.0527,  1.9455,  ..., -3.9736, -1.8403, -4.3696],\n",
       "         [-4.5610,  1.4004,  1.3086,  ..., -3.9020, -0.3275, -4.7346],\n",
       "         [-4.3410,  3.8885,  1.9976,  ..., -4.0874, -1.8874, -4.3304]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-4.2005,  0.1515,  0.2446,  ...,  0.0910,  2.7407, -4.4495],\n",
       "         [-4.1492,  1.0351,  1.4469,  ..., -3.4674, -0.2604, -4.7569],\n",
       "         [-4.8629,  1.3600,  1.4060,  ..., -3.8780,  0.0723, -4.7853],\n",
       "         ...,\n",
       "         [-4.5941,  1.1830,  1.3325,  ..., -3.7379,  0.0297, -4.7032],\n",
       "         [-4.2414,  1.2856,  1.3436,  ..., -3.6047, -0.4564, -4.4350],\n",
       "         [-4.6850,  0.9282,  1.3033,  ..., -2.1104,  0.4630, -4.9510]],\n",
       "\n",
       "        [[-4.4164, -0.0658,  0.3201,  ..., -0.3671,  3.3564, -3.9622],\n",
       "         [-4.1292,  1.3372,  1.4953,  ..., -3.4166, -0.7463, -4.7114],\n",
       "         [-4.7052,  1.3157,  1.3936,  ..., -4.0190,  0.2975, -4.6564],\n",
       "         ...,\n",
       "         [-4.4832,  1.3035,  1.4376,  ..., -4.2050, -0.2360, -4.5661],\n",
       "         [-4.4664,  1.3118,  1.5330,  ..., -3.6296, -0.5569, -4.6761],\n",
       "         [-4.7414,  0.5893,  0.8383,  ..., -0.9519,  1.4580, -4.9828]],\n",
       "\n",
       "        [[-3.9494,  4.2212,  1.9041,  ..., -3.8052, -2.5212, -4.0759],\n",
       "         [-4.4133,  1.1354,  1.4374,  ..., -3.4848, -0.4950, -4.6814],\n",
       "         [-4.6865,  0.8727,  1.2515,  ..., -3.4879,  0.5189, -4.7410],\n",
       "         ...,\n",
       "         [-4.4783,  1.5222,  1.5332,  ..., -4.0697, -0.2264, -4.5505],\n",
       "         [-4.4256,  1.1915,  1.2660,  ..., -3.8293, -0.1805, -4.6044],\n",
       "         [-4.7338,  0.5633,  0.9032,  ..., -1.3103,  1.4338, -5.0270]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(src,tgt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that if your run these dummy inputs through the model multiple times, the numbers change. This is because we have given this instance of the model dropout = 0.1 -- which randomly chooses 10% of the cofficients to turn to zero every time it runs. If we want to remove this and have a static model, we simple need to instantiate a new model without dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_static = KernelModel(vocab_size,d_model,d_ffn,nhead,encoder_layers,decoder_layers,dropout=0)\n",
    "model_static.load_state_dict(torch.load(\"../models/test_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-4.5825,  1.0400,  1.3531,  ..., -3.3819,  0.0482, -4.8566],\n",
       "         [-4.3006,  1.3465,  1.4643,  ..., -3.7228, -0.6543, -4.5880],\n",
       "         [-4.5805,  1.1599,  1.4061,  ..., -3.7923,  0.2424, -4.7464],\n",
       "         ...,\n",
       "         [-4.5218,  1.2905,  1.4187,  ..., -3.7169, -0.2926, -4.7285],\n",
       "         [-4.8254,  0.3129,  0.3601,  ..., -0.7590,  2.9198, -4.5413],\n",
       "         [-4.1157,  1.3464,  1.4823,  ..., -3.5534, -0.8285, -4.5053]],\n",
       "\n",
       "        [[-4.5825,  1.0400,  1.3531,  ..., -3.3819,  0.0482, -4.8566],\n",
       "         [-4.3671,  1.2937,  1.4176,  ..., -3.5659, -0.5293, -4.6208],\n",
       "         [-4.0769,  4.3929,  2.0561,  ..., -4.2985, -2.1331, -4.1152],\n",
       "         ...,\n",
       "         [-4.5171,  1.2208,  1.4519,  ..., -3.7090, -0.2367, -4.7048],\n",
       "         [-4.7472,  1.1454,  1.3040,  ..., -2.7055,  0.5830, -4.9792],\n",
       "         [-4.1230,  1.4266,  1.5192,  ..., -3.6172, -0.8433, -4.5173]],\n",
       "\n",
       "        [[-3.9264,  4.2062,  2.0580,  ..., -3.9244, -2.3467, -4.0382],\n",
       "         [-4.6810,  0.1557,  0.3756,  ..., -0.3708,  2.6661, -4.5823],\n",
       "         [-4.6230,  1.2404,  1.4344,  ..., -3.8420,  0.0159, -4.7874],\n",
       "         ...,\n",
       "         [-4.0653,  4.2134,  2.0428,  ..., -4.1236, -2.3337, -4.1687],\n",
       "         [-4.4187,  1.3412,  1.4184,  ..., -3.7782, -0.4614, -4.6636],\n",
       "         [-3.9311,  3.2892,  1.9848,  ..., -4.0744, -2.1967, -4.2717]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-4.5360, -0.0551,  0.3537,  ..., -0.0997,  2.9998, -4.4590],\n",
       "         [-4.2831,  1.3151,  1.4512,  ..., -3.7265, -0.6212, -4.5812],\n",
       "         [-4.6903,  1.2153,  1.3772,  ..., -3.6736,  0.0978, -4.8196],\n",
       "         ...,\n",
       "         [-4.4906,  1.2900,  1.4225,  ..., -3.7300, -0.2422, -4.7159],\n",
       "         [-4.3926,  1.3548,  1.4273,  ..., -3.8150, -0.4992, -4.6403],\n",
       "         [-4.8787,  0.7380,  0.9500,  ..., -1.4867,  1.4540, -4.9217]],\n",
       "\n",
       "        [[-4.5271, -0.0592,  0.3362,  ..., -0.0708,  3.0213, -4.4419],\n",
       "         [-4.3207,  1.3185,  1.4240,  ..., -3.7030, -0.5906, -4.6342],\n",
       "         [-4.6876,  1.2120,  1.3770,  ..., -3.6734,  0.1054, -4.8188],\n",
       "         ...,\n",
       "         [-4.4601,  1.3008,  1.4629,  ..., -3.7986, -0.3721, -4.6950],\n",
       "         [-4.3790,  1.2683,  1.4328,  ..., -3.7638, -0.4062, -4.5796],\n",
       "         [-4.8615,  0.6825,  0.9046,  ..., -1.3692,  1.5625, -4.9101]],\n",
       "\n",
       "        [[-3.8567,  4.2942,  2.0334,  ..., -3.9599, -2.4253, -3.9564],\n",
       "         [-4.3256,  1.3096,  1.4213,  ..., -3.6967, -0.5798, -4.6374],\n",
       "         [-4.6415,  1.1312,  1.4009,  ..., -3.7664,  0.1656, -4.7714],\n",
       "         ...,\n",
       "         [-4.5124,  1.3251,  1.4404,  ..., -3.7805, -0.3418, -4.7623],\n",
       "         [-4.4016,  1.3331,  1.4140,  ..., -3.7496, -0.4014, -4.6099],\n",
       "         [-4.8107,  0.4377,  0.8199,  ..., -1.3572,  1.8013, -4.9217]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_static(src,tgt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the outputs are fixed - that means that has the tokens are run through the model, all the cofficients are being utilized. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to train a new model. The key here is that we need to teach the model how to predict the next token given the input sequence, and all tokens upto the next one. To do this we've implemented a training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see this in action, let's load in some hypers for a new model, model_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters loaded :-)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# ---------------------\n",
    "# Build a trainable model with the same hyperparameters above.\n",
    "# ---------------------\n",
    "\n",
    "model_train = KernelModel(vocab_size,d_model,d_ffn,nhead,encoder_layers,decoder_layers,dropout)\n",
    "\n",
    "# ---------------------\n",
    "# Traing Functions & Device\n",
    "# ---------------------\n",
    "epochs = 3\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN_ID)\n",
    "optimizer = optim.Adam(model_train.parameters(), lr=1e-3)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_train.to(device)\n",
    "\n",
    "print(\"parameters loaded :-)\")\n",
    "\n",
    "# ---------------------\n",
    "# Batch the dataloader to desired specs\n",
    "# ---------------------\n",
    "\n",
    "batchsize = 5\n",
    "dataloader = DataLoader(data,batch_size=batchsize,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training function takes in a model (which could have bee preloaded with parameters), along with training specific objects (device, criterion, optimizer, dataloader, epochs), and then once the training has been completed, it saves the model to \"example_model.pth\" in the models directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 1000/1000 [00:38<00:00, 25.71it/s, loss=2.08]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Avg. Loss: 1.3886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 1000/1000 [00:40<00:00, 24.97it/s, loss=1.82]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Avg. Loss: 1.2766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 1000/1000 [00:43<00:00, 22.97it/s, loss=1.6] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Avg. Loss: 1.2755\n",
      "Training completed in 122.51 seconds\n",
      "Model saved to ../models/example_model.pth\n"
     ]
    }
   ],
   "source": [
    "train(model_train, device, criterion, optimizer, dataloader, epochs, \"example_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model as been trained, we can start testing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "The obvious first step is to check how it does with building reductions from the input integral. For this we have built a wrapper model class, ProjectModel, which unlike the KernelModel, only takes an input string, and then does auto-regressive encoding to build an output string. (EDIT: Still need to git push the token and sequence testing functions)\n",
    "\n",
    "Let's see this in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from testing import *\n",
    "\n",
    "fullmodel = ProjectionModel(model_train,tokenizer,max_length = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a small example model (with d_model=16), and we're running it on the tenniscourt integrals, so obviously performance is going to be quite bad. But it's worth showing how it ProjectionModel, which is our honest sequence-to-sequence model, performs once the KernelModel has been trained. \n",
    "\n",
    "Take the following integral as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{010,0006010220100106,13078100,100000000,0,,7827701270030001293306,176021200007,72207162660276020002'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullmodel('{2,1,1,1,0,-1,1,0,1,0,-1,0,0,0,0}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, since we used our model_train KernelModel with dropout = 0.1, the outputs generated above will be randomly constructed. We can resolve this by instantiating a static model without dropout, and then using that to define a new inference model based on the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_static =  KernelModel(vocab_size,d_model,d_ffn,nhead,encoder_layers,decoder_layers,dropout=0)\n",
    "model_static.load_state_dict(torch.load(\"../models/example_model.pth\"))\n",
    "\n",
    "fullmodel_static = ProjectionModel(model_static,tokenizer,max_length = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: {0111110111100001000,1,17,0,1600000000007,,,,77771\n",
      "True: {4988942163698054660,0,11073643427184746115,12786563879566983934,11297505512155254377,0,16071282845234746120,0,3877329585571272712,0,0,0,0,0,2351088370595720278,0,0,0,0,0,0,0,9485612509064294181,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0}\n",
      "Prediction: {0111110111100001000,1,,,0,1,177000000007,,,,77771\n",
      "True: {10860739550675742401,1575801467500097271,15432346270711039692,0,0,16908624229613386083,0,0,0,6908755195386501367,0,12230655570764797522,9632817245834476874,15588438248097191445,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0}\n",
      "Prediction: {0111110111000001000,1,,,0,07600000000007,,,,77771\n",
      "True: {0,0,15292746396285261898,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0}\n",
      "Prediction: {0111110111100001000,1,,,07,1600000000007,,,,77771\n",
      "True: {1428586150493445959,0,3674960714066933736,0,0,0,0,0,14658749303094349744,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0}\n",
      "Prediction: {0111110111100001000,1,,,07,1600000000007,,,,77771\n",
      "True: {0,0,0,0,9223372036854776951,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0}\n",
      "Prediction: {0111110111100001000,1,,,0,1,177000000007,,,,77771\n",
      "True: {4119490293016467252,0,0,0,0,13358472046223467936,0,0,0,14899208181494524009,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0}\n",
      "Prediction: {0111110111000001000,1,,,07,1600000000007,,,,77771\n",
      "True: {0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,13082053270794834016,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0}\n",
      "Prediction: {01111101110000001010,1,,,0,1600000000007,,,,77771\n",
      "True: {0,0,10803031392760155729,0,0,7643712680949395828,0,17429478275580625081,0,0,0,12512021525483236742,0,0,0,0,0,0,0,0,0,0,0,0,0,0,9698287635897948849,0,0,0,0,0,0,0,0,0,0,0,0,0,0}\n",
      "Prediction: {0111110111000001000,1,,,0,1,170000000007,,,,77771\n",
      "True: {0,564642635976363147,0,0,0,13781688424125694965,0,0,0,0,5221636625216243394,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0}\n",
      "Prediction: {0111110111100001000,1,,,0,1,177000000007,,,,77771\n",
      "True: {11691005900011820821,13675810562794616619,14815658119761407052,413359208144825206,0,6976681171527154389,0,0,15334367923005133736,15405523242857016892,14196897250313146145,6458859331295088819,12578021573714515838,0,0,0,0,2427283798224660009,2916895926956697296,1121247878009170105,6589650103146276536,0,0,0,0,0,0,0,11404397333319010507,10937858390205017913,0,0,0,0,0,0,0,0,0,0,0}\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f\"Prediction: {fullmodel_static(data.input_strings[i])}\")\n",
    "    print(f\"True: {data.output_strings[i]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
